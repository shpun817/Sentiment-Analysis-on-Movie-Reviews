---
title: "ISOM3390 Final Project"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, tidy = TRUE)
```

Before proceeding to the exercise, make sure the `tidyverse` is installed in your lab computer.

```{r}
library(tidyverse)
library(rvest)
library(RSelenium)
library(tidytext)
```

First, r un the command
java -Dwebdriver.chrome.driver=chromedriver.exe -jar selenium-server-standalone-3.141.59.jar
to start the server.

```{r}
# Connect to the running server
remDr <- remoteDriver(remoteServerAddr = "localhost", port = 4444L,
browserName = "chrome") # "firefox", "internet explorer", "iphone", etc.
#str(remDr, max.level = 1)

# Start the browser
remDr$open(silent = TRUE)

# Get the status of the server
remDr$getStatus() %>% str()

# Navigate to the url
remDr$navigate("https://www.imdb.com/chart/moviemeter?sort=rk,asc&mode=simple&page=1")
```

For each movie, control the server to click into the movie title and collect necessary data

```{r}
# First, create a tibble with movie titles and their hyperlinks
page_source <- remDr$getPageSource()[[1]] %>% read_html()
movies_table <- page_source %>% html_node(".lister-list")
movies_titles <- movies_table %>% html_nodes("a") %>% html_text(trim = TRUE) %>% .[seq(2, 200, 2)]
movies_urls <- movies_table %>% html_nodes("a") %>% html_attr("href") %>% .[seq(2, 200, 2)] %>% str_c("https://www.imdb.com", .)

movies <- tibble(title = movies_titles, url = movies_urls)
movies %>% head
```

```{r}
# Create a tibble to store user reviews
reviews <- tibble(movie = character(), rating = numeric(), review_header = character(), review = character())

# Next, use a for loop along the movies tibble to collect 100 user reviews for each movie
#for (i in seq_along(movies$title)) {
for (i in 1:3) { 
  # Extract the id that IMDb assigns to each movie
  movie_id <- movies$url[i] %>% str_extract("/title/.+/")
  
  # Navigate to the user reviews page
  reviews_url <- movie_id %>% str_c("https://www.imdb.com", ., "reviews?ref_=tt_urv")
  remDr$navigate(reviews_url)
  
  Sys.sleep(3)
  
  # Locate the hide spoilers checkbox
  hideSpoilers <- remDr$findElement(using = "css", value = ".lister-widget-sprite.lister-checkbox")
  #class(hideSpoilers)
  hideSpoilers$clickElement() # Tick the checkbox to hide spoilers
  
  Sys.sleep(3)
  
  # Find the total number of reviews
  page_source <- remDr$getPageSource()[[1]] %>% read_html()
  num_reviews <- page_source %>% html_node(".lister") %>% html_node(".header") %>% html_nodes("span") %>% .[1] %>% html_text(trim = TRUE) %>% str_sub(end = -9L) %>% gsub(",", "", .) %>% as.numeric()
  
  # Determine the number of clicks on the show more button.
  # At start the page shows 25 reviews (or less).
  # Each click loads 25 more.
  num_clicks <- 3
  if (num_reviews <= 25) {
    num_clicks <- 0
  } else if (num_reviews <= 50) {
    num_clicks <- 1
  } else if (num_reviews <= 75) {
    num_clicks <- 2
  } else {
    num_clicks <- 3
  }
  
  # Perform the determined number of clicks
  for (click in 1:num_clicks) {
    # Locate the loadmore button
    loadMore <- remDr$findElement(using = "css", value = ".ipl-load-more__button")
    loadMore$clickElement()
    Sys.sleep(3) # Wait for loading
  }
  
  # Expand all long review
  expanders <- remDr$findElements(using = "css", value = ".expander-icon-wrapper.show-more__control")
  for (j in seq_along(expanders)) {
    expanders[[j]]$clickElement()
  }
  
  # Locate the list containing all user reviews
  page_source <- remDr$getPageSource()[[1]] %>% read_html()
  whole_list <- page_source %>% html_node(".lister-list") %>% html_nodes(".lister-item.mode-detail.imdb-user-review.collapsable")
  
  # For each piece of user review, collect the rating and the review
  review_count <- 1 # when this count reaches 101, break the loop
  for (j in seq_along(whole_list)) {
    if (review_count >= 101) {
      break
    }
    
    # Obtain the rating
    rating <- NA
    # Check if a rating exists
    if (whole_list[j] %>% html_nodes(".rating-other-user-rating") %>% length > 0) {
      rating <- whole_list[j] %>% html_node(".rating-other-user-rating") %>% html_node("span") %>% html_text(trim = TRUE) %>% as.numeric()
    }
    
    # Obtain the review header
    header <- NA
    # Check if a header exists
    if (whole_list[j] %>% html_nodes(".title") %>% length > 0) {
      header <- whole_list[j] %>% html_node(".title") %>% html_text(trim = TRUE)
    }
    
    # Obtain the review content
    content <- NA
    # Check if content exists (play safe)
    if (whole_list[j] %>% html_nodes(".text.show-more__control") %>% length > 0) {
      content <- whole_list[j] %>% html_node(".text.show-more__control") %>% html_text(trim = TRUE)
    }
    
    reviews <- reviews %>% add_row(movie = movies$title[i], rating = rating, review_header = header, review = content)
    
    review_count <- review_count + 1
  }
  
}

```

In today's lab, we are going to use the functions provided by the `rvest` package to scrape data available on a Amazon product web page. 


We start with a [seed page](https://www.amazon.com/dp/B071X9KT1D/), and find the product pages to be scraped sequentially by looking at items displayed in the 1st "Customers who bought this item also bought" carousel. Each product item in this carousel are structured as an HTML list item node delineated by the <li> tag with `class=".a-carousel-card.aok-float-left"`.

The following commands download the source code, and extract these list item nodes for use.


```{r}
webpage <- read_html("https://www.amazon.com/dp/B071X9KT1D/")

(results <- html_nodes(webpage, ".a-carousel-card.aok-float-left") %>% .[1:6]) # subset the items from the 1st carousel

# If there are not enough items in this carousel, you will see:
# Error in nodes_duplicated(nodes) : Expecting an external pointer: [type=NULL].


results[2] %>% toString() %>% writeLines() # print the HTML source code of the 1st item for inspection
```

Note that the title of the corresponding book (i.e., *Regression Modeling with Actuarial and Financial Applications (International Series on Actuarial Science)*) can be spotted at several places in the above code; for example, the `alt` attribute of a `<img>` tag, the text (truncated) enclosed by a pair of <div> tags of both the `p13n-sc-truncate` class and `p13n-sc-line-clamp-3` classe, etc.

Let's inspect the code passage below closely:

```{html eval=F}
<div class="p13n-sc-truncate p13n-sc-line-clamp-3 p13n-sc-truncate-desktop-type2" aria-hidden="true" data-rows="3">
            Discovering Statistics Using R
        </div>
```

Note that `p13n-sc-truncate p13n-sc-line-clamp-3 p13n-sc-truncate-desktop-type2"` contains a space in the value part. We know that a space, when used to construct a css selector, means selecting the second node/element inside the first node/element, as the second node/element is the child of the first node/element in the document's hirerarchy.

Here, the space means a different thing. When spaces are used in the class value, it means the element belongs to  `p13n-sc-truncate` class, `p13n-sc-truncate-desktop-type2` class and the `p13n-sc-line-clamp-3` class. Therefore, we can tell the program to identify this element with either one. Or if we want to match only elements that are of both classes (so as to skip those elements belonging to either class), using `".p13n-sc-truncate.p13n-sc-line-clamp-3.p13n-sc-truncate-desktop-type2"` without space as the selector. So we can extract the (truncated) titles of all the 6 books by running the following

```{r}
(titles <- results%>%html_node(".p13n-sc-truncate.p13n-sc-line-clamp-3.p13n-sc-truncate-desktop-type2") %>% html_text(trim = TRUE))
```

Provide the code to extract authors, ratings, and prices for the 6 books and convert them from string form to appropriate types.  Please note that for the authors, some of them are links and some of them are plain text. The classes in the webpage are different. `ratings`, it is confirmed that the format is in `X.X out of 5 stars`, e.g. `3.0 out of 5 stars`. You are going to use `str_sub()` to get the value `X.X`.

### Task1
```{r task1, echo=F}
# edit here

(authors <- results %>% html_node(".a-row.a-size-small") %>% html_text(trim = TRUE))
(ratings <- results %>% html_node(".a-icon-alt") %>% html_text(trim = TRUE) %>% str_sub(1, 3) %>% as.numeric)
(prices <- results %>% html_node(".p13n-sc-price") %>% html_text(trim = TRUE))

```
 

The URLs of the product webpages for the 6 books can be extracted by using `html_attr()`. Provide your code below:

### Task2
```{r task2, echo=F}

(urls <- results %>% html_node(".a-link-normal") %>% html_attr("href"))

```

Comparing these urls with the url of the seed page (<https://www.amazon.com/dp/B071X9KT1D/>) reveals that the raw are relative ones. Besides, some part of them are used for the purpose other than locating the product webpages (e.g., tracking referring sources). We want to remove the irrelevant part and use the remaining (the part contained in the value part of the substring `"/dp/B00H7WPACC/"`) to form absolute urls for later use. Name your result `complete_urls`


### Task3
```{r task3, echo=F}

(complete_urls <- urls %>% str_extract("/dp/.*/ref") %>% str_sub(1, -4) %>% str_c("https://www.amazon.com", .))

```



Now we are going to scrape reviews for these books by visiting their webpages. Provide your code below to extract the textual reviews displayed initially for the *1st book* (no need to use the selenium server to load all reviews)


### Task4
```{r task4, echo=F}
# edit here
 
(reviews <- read_html(complete_urls[1]) %>% html_nodes(".a-expander-content.reviewText.review-text-content.a-expander-partial-collapse-content") %>% html_text(trim = TRUE))

```


