---
title: "ISOM3390 Final Project"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, tidy = TRUE)
```

# Group 10
### PUN, Sanford Harrap 20607555
### WONG, Wing Kin 20504727



# Setup
```{r}
library(tidyverse)
library(rvest)
library(RSelenium)
library(tidytext)
```

# Scraping

First, run the command
java -Dwebdriver.chrome.driver=chromedriver.exe -jar selenium-server-standalone-3.141.59.jar
to start the server.

```{r eval=FALSE}
# Connect to the running server
remDr <- remoteDriver(remoteServerAddr = "localhost", port = 4445L,
browserName = "chrome") # "firefox", "internet explorer", "iphone", etc.
#str(remDr, max.level = 1)

# Start the browser
remDr$open(silent = TRUE)

# Get the status of the server
remDr$getStatus() %>% str()

# Navigate to the url
remDr$navigate("https://www.imdb.com/chart/moviemeter?sort=rk,asc&mode=simple&page=1")
```

For each movie, control the server to click into the movie title and collect necessary data

```{r eval=FALSE}
# First, create a tibble with movie titles and their hyperlinks
page_source <- remDr$getPageSource()[[1]] %>% read_html()
movies_table <- page_source %>% html_node(".lister-list")
movies_titles <- movies_table %>% html_nodes("a") %>% html_text(trim = TRUE) %>% .[seq(2, 200, 2)]
movies_urls <- movies_table %>% html_nodes("a") %>% html_attr("href") %>% .[seq(2, 200, 2)] %>% str_c("https://www.imdb.com", .)

movies <- tibble(title = movies_titles, url = movies_urls)
movies %>% head
```

Store the reviews of each movie into a big tibble "reviews".

```{r eval=FALSE}
# Create a tibble to store user reviews
reviews <- tibble(movie = character(), rating = numeric(), review_header = character(), review = character())

# Next, use a for loop along the movies tibble to collect 100 user reviews for each movie
for (i in seq_along(movies$title)) {
  # Extract the id that IMDb assigns to each movie
  movie_id <- movies$url[i] %>% str_extract("/title/.+/")
  
  # Navigate to the user reviews page
  reviews_url <- movie_id %>% str_c("https://www.imdb.com", ., "reviews?ref_=tt_urv")
  remDr$navigate(reviews_url)
  
  # Wait for loading
  Sys.sleep(3)
  
  # Locate the hide spoilers checkbox
  hideSpoilers <- remDr$findElement(using = "css", value = ".lister-widget-sprite.lister-checkbox")
  #class(hideSpoilers)
  hideSpoilers$clickElement() # Tick the checkbox to hide spoilers
  
  Sys.sleep(3)
  
  # Find the total number of reviews
  page_source <- remDr$getPageSource()[[1]] %>% read_html()
  num_reviews <- page_source %>% html_node(".lister") %>% html_node(".header") %>% html_nodes("span") %>% .[1] %>% html_text(trim = TRUE) %>% str_extract(".+ ") %>% str_sub(end = -2) %>% gsub(",", "", .) %>% as.numeric()
  
  if (num_reviews <= 0) {
    next
  }
  
  # Determine the number of clicks on the show more button.
  # At start the page shows 25 reviews (or less).
  # Each click loads 25 more.
  num_clicks <- 3
  if (num_reviews <= 25) {
    num_clicks <- 0
  } else if (num_reviews <= 50) {
    num_clicks <- 1
  } else if (num_reviews <= 75) {
    num_clicks <- 2
  } else {
    num_clicks <- 3
  }
  
  if (num_clicks > 0) {
    # Perform the determined number of clicks
    for (click in 1:num_clicks) {
      # Locate the loadmore button
      loadMore <- remDr$findElement(using = "css", value = ".ipl-load-more__button")
      loadMore$clickElement()
      Sys.sleep(3) # Wait for loading
    }
  }
  
  # Expand all long review
  expanders <- remDr$findElements(using = "css", value = ".expander-icon-wrapper.show-more__control")
  if (expanders %>% length > 0) {
    for (j in seq_along(expanders)) {
      expanders[[j]]$clickElement()
    }
  }
  
  # Locate the list containing all user reviews
  page_source <- remDr$getPageSource()[[1]] %>% read_html()
  whole_list <- page_source %>% html_nodes(".lister-list")
  if (whole_list %>% length == 0) { # List not found
    next
  }
  whole_list <- whole_list[1] %>% html_nodes(".lister-item.mode-detail.imdb-user-review.collapsable")
  
  if (whole_list %>% length == 0) { # No review elements in the list
    next
  }
  
  # For each piece of user review, collect the rating and the review
  review_count <- 1 # when this count reaches 101, break the loop
  for (j in seq_along(whole_list)) {
    if (review_count >= 101) {
      break
    }
    
    # Obtain the rating
    rating <- NA
    # Check if a rating exists
    if (whole_list[j] %>% html_nodes(".rating-other-user-rating") %>% length > 0) {
      rating <- whole_list[j] %>% html_node(".rating-other-user-rating") %>% html_node("span") %>% html_text(trim = TRUE) %>% as.numeric()
    }
    
    # Obtain the review header
    header <- NA
    # Check if a header exists
    if (whole_list[j] %>% html_nodes(".title") %>% length > 0) {
      header <- whole_list[j] %>% html_node(".title") %>% html_text(trim = TRUE)
    }
    
    # Obtain the review content
    content <- NA
    # Check if content exists (play safe)
    if (whole_list[j] %>% html_nodes(".text.show-more__control") %>% length > 0) {
      content <- whole_list[j] %>% html_node(".text.show-more__control") %>% html_text(trim = TRUE)
    }
    
    reviews <- reviews %>% add_row(movie = movies$title[i], rating = rating, review_header = header, review = content)
    
    review_count <- review_count + 1
  }
  
}

```

### Not Enough Reviews
It is worth mentioning that even though we are tasked to collect 100 reviews per movie, but the total number of reviews we extracted is less than 10000. This is due to the fact that some movies have less than 100 reviews or even no reviews at all. In our analysis, we collect up to 100 reviews from each movie, without putting too much thoughts into those with under 100, as it does not affect the sentiment analysis later on. (In short, for those movies with less than 100 reviews, we simply extracted all.)

Write the reviews tibble into a csv file.

```{r eval=FALSE}
reviews %>% write.csv("reviews.csv")
```

That concludes the process of scraping reviews.

# Analysis

Read the reviews from the csv file.

```{r}
reviews <- read.csv("reviews.csv")
```


There are 4334 positive reviews and 2756 negative reviews, which is a slightly imbalance distribution. Let's sample 2500 Positive reviews and 2500 negative reviews and store it to reviews_sample

```{r}
reviews_pos <- reviews %>% filter(rating >= 6) %>% head(2500)
reviews_neg <- reviews %>% filter(rating <= 5) %>% head(2500)

reviews_sample <- rbind(reviews_pos, reviews_neg)

reviews_sample %>% head
```



Now, we have to do some preprocessing. Let's tokenize and remove the stop-word in the review

```{r}
reviews_sample_token <- reviews_sample %>% unnest_tokens(word,review) %>% filter(!str_detect(word, "^[0-9]*$"))
reviews_sample_no_stopword <- reviews_sample_token %>% anti_join(stop_words, by= "word")
reviews_sample_bing <- reviews_sample_no_stopword %>% inner_join(get_sentiments("bing"), by= "word")
reviews_sample_bing %>% head
```

We have a bag of words now. Let's predict the sentiment according to those word, and evaluate the outcomes by contrasting with actual rating.
Since some of the reviews just contain a bunch of stopwords, those reviews is removed in the preprocessing.

```{r}
reviews_predict <- reviews_sample_bing %>% count(X, movie, review_header, rating, sentiment) %>% pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% mutate(sentiment = round(positive * 10 / (positive + negative)) , diff = sentiment - rating  )

reviews_predict %>% head
```

Let's evaluate the outcomes by contrasting with actual rating.
677 reviews are perfectly predicted.

```{r}
reviews_predict %>% filter(diff == 0) %>% inner_join(reviews, by= c("X", "movie",  "review_header", "rating") ) %>% select("X","review", "rating", "negative", "positive", "sentiment", "diff"  ) %>% head
```


```{r}
reviews_predict %>% filter(diff < -5) %>% inner_join(reviews, by= c("X", "movie",  "review_header", "rating") ) %>% select("X","review", "rating", "negative", "positive", "sentiment", "diff"  ) %>% head
```


## Evaluation

We identify 5 causes for the inaccurate prediction.

The current metric of sentiment for each review is very simple: the percentage of positive words in the review, rid of any stop words. We observe the trend that many reviewers gave some criticism (containing negative words) even though they gave 10/10. Under the current metric, negative words create huge impacts on the sentiment prediction.


The scale of sentiment scores currently is relatively intolerant to differences. For instance, a score of 9 and a score of 10 are inherently similar to humans, but the current system does not tolerate the small difference.
Let's try to evaluate with the ROC score. Here, we do a binary classification. If the predict score is higher than 5, then it is "positive", otherwise, it is "negative", thus increasing the tolerance. 
```{r}
library(ROCR)
pred <- prediction(reviews_predict$sentiment, ifelse(reviews_predict$rating>5,1,0))
perf <- performance(pred,"tpr","fpr")
plot(perf,colorize=TRUE)

```

As we can see from the graph, the Area Under Curve (AUC) under this binary scale is large, implying a high accuracy.

Another reason is that English words convey different meanings depending on the context. Reviewers might have used negative words without knowing it, or vice versa. A good example is the word "funny". A user might have praised a comedy movie for it being funny, but actually funny is considered to be a negative word, thus resulting in a false negative analysis. 

In the English language, double negative is a common way to express appreciation (e.g. "not bad"), but it is very tricky to the program, as the program considers each word separately. Therefore, the program can be improved by using n-grams: consider more consecutive words instead of only one at a time. This way the grammar and order of words, which are particularly important in languages such as English, will be considered in the analysis.


```{r}
freq_words <- reviews_sample_bing %>% select("word" , "sentiment" , "rating") %>% group_by(word , sentiment, rating) %>% summarise("count" = n())
freq_words <- freq_words[order(freq_words$count , decreasing = TRUE),]
```

The 5 most commonly-used negative word.

```{r}
freq_words_neg <-freq_words %>% filter(sentiment == "negative") %>% select("word" , "sentiment" , "count") %>% summarise("sum" = sum(count))
freq_words_neg <- freq_words_neg[order(freq_words_neg$sum , decreasing = TRUE),]
freq_words_neg <- freq_words_neg %>% head(5)
freq_words_neg
```

The 5 most commonly-used positive word.

```{r}
freq_words_pos <- freq_words %>% filter(sentiment == "positive") %>% select("word" , "sentiment" , "count") %>% summarise("sum" = sum(count))
freq_words_pos <- freq_words_pos[order(freq_words_pos$sum , decreasing = TRUE),]
freq_words_pos <- freq_words_pos %>% head(5)
freq_words_pos
```
Below are the graph

```{r}

data <- freq_words %>% inner_join(freq_words_pos, by="word", "sentiment") %>% mutate(weight = count / sum)
ggplot(data, aes(rating, weight)) + geom_point() + geom_line() + theme_bw() + facet_wrap(~ word, ncol = 3, scales = "free_x") + ylab("Pr(c|w)") + xlab("Rating") + scale_x_continuous(breaks = 1:10)
```



```{r}
data <- freq_words %>% inner_join(freq_words_neg, by="word", "sentiment") %>% mutate(weight = count / sum)
ggplot(data, aes(rating, weight)) + geom_point() + geom_line() + theme_bw() + facet_wrap(~ word, ncol = 3, scales = "free_x") + ylab("Pr(c|w)") + xlab("Rating") + scale_x_continuous(breaks = 1:10)
```


